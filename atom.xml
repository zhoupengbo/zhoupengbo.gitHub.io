<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>漫泊今生 扶垚而上</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-02-24T12:02:32.476Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>zhou.pengbo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>解惑大学生：计算机专业毕业以后都干嘛？希望这张图可以帮到你~</title>
    <link href="http://yoursite.com/2020/02/24/%E8%A7%A3%E6%83%91%E5%A4%A7%E5%AD%A6%E7%94%9F%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E6%AF%95%E4%B8%9A%E4%BB%A5%E5%90%8E%E9%83%BD%E5%B9%B2%E5%98%9B%EF%BC%9F%E5%B8%8C%E6%9C%9B%E8%BF%99%E5%BC%A0%E5%9B%BE%E5%8F%AF%E4%BB%A5%E5%B8%AE%E5%88%B0%E4%BD%A0~/"/>
    <id>http://yoursite.com/2020/02/24/%E8%A7%A3%E6%83%91%E5%A4%A7%E5%AD%A6%E7%94%9F%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E6%AF%95%E4%B8%9A%E4%BB%A5%E5%90%8E%E9%83%BD%E5%B9%B2%E5%98%9B%EF%BC%9F%E5%B8%8C%E6%9C%9B%E8%BF%99%E5%BC%A0%E5%9B%BE%E5%8F%AF%E4%BB%A5%E5%B8%AE%E5%88%B0%E4%BD%A0~/</id>
    <published>2020-02-24T10:30:21.000Z</published>
    <updated>2020-02-24T12:02:32.476Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>前言：</strong>小外甥今年上大二，专业计算机科学与技术，前几日聊天跟我说不知道自己现在学的知识是干嘛用的，毕业以后干神马，神马是前端神马是后端呢？我听后跟他说等我画张图给你看吧。这篇文章分享给所有计算机专业的大学生，希望能帮到祖国的花朵们~</p></blockquote><a id="more"></a><p>世界那么大，我们都想去看看。其实，每一个专业背后也都是一片汪洋大海，每个人都在海里漂流，沿着自己的航线，渴望逐向前方的小岛~</p><p>我自己呢是做软件开发的，具体说呢应该是大数据开发，前端后端基本也都做过，虽然不深但也大概了解。硬件和网络相对了解少一些，所以着墨也就少了点，怕误人子弟。</p><p>我凭自己的感受将计算机分为软件、硬件及网络三大方向，又着重对软件行业细分为功能型、资源型、保障型、管理型、企业型多个维度，主要是想让大家看的清晰一些，但毕竟技术是相互依赖的，你中有我，我中有你，不可能完全分隔。</p><p>下面我们先来看一下这片大海吧，希望从大海中可以找到自己心中的那个小岛~</p><p><img src="https://i.loli.net/2020/02/24/urKhNQZYL3jftzv.png" alt="计算机专业就业方向一览"></p><p>看完这幅图，还是想对祖国的花朵们灌一些鸡汤喝喝，哈哈~</p><p>首先，还是希望大家能提升自己的动手能力，要有探索精神，毕竟我们是靠手来吃饭的。学校学习的知识，我们要应用到实战中去，自己可以多找找项目，练习练习，练着练着也就不疑惑了。所以，大家一定要多动手~</p><p>其次，这幅图可能只是一个粗略的就业方向图，其实在工作中，行业会分的很细很细。比如爬虫工程师、推荐系统工程师、k-v 存储开发工程师等等，这些是面向业务模型的；还有像Redis高级开发工程师、Redis专家、Spring专家等等，这些是面向某门技术或框架的。</p><p>大家没必要过分担心，这些也都是上图某个细分方向的衍化而已，等你们接触到业务或项目的时候就会知道的。其实还是要做好第一点，上帝不会亏待勤奋的孩子的。所以，大家一定不要太担心~</p><p>最后，想说的是，不管我们做什么，都是一条精进之路。三百六十行，行行出状元，我们这行业当然也不例外。不管我们选择什么方向，都要经历初级、中级、高级等一路的磨难，然后成为某个领域的专家或是架构师或是管理者或是被淘汰。所以，大家一定要知努力~</p><p>如果大家还有什么疑问，可以进入公众号【chan-ke】首页，点击【点我】【加好友】添加我的个人微信哦~ 请记得填写备注信息哦，陌生人是不予通过的~</p><p>在这里向大家问好，祝大家身体健康，开心快乐，学习更上一层楼~</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;前言：&lt;/strong&gt;小外甥今年上大二，专业计算机科学与技术，前几日聊天跟我说不知道自己现在学的知识是干嘛用的，毕业以后干神马，神马是前端神马是后端呢？我听后跟他说等我画张图给你看吧。这篇文章分享给所有计算机专业的大学生，希望能帮到祖国的花朵们~&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="经验贴" scheme="http://yoursite.com/categories/%E7%BB%8F%E9%AA%8C%E8%B4%B4/"/>
    
    
      <category term="2020" scheme="http://yoursite.com/tags/2020/"/>
    
      <category term="计算机" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
      <category term="就业" scheme="http://yoursite.com/tags/%E5%B0%B1%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>全方位认识HBase：一个值得拥有的NoSQL数据库（一）</title>
    <link href="http://yoursite.com/2020/02/24/%E5%85%A8%E6%96%B9%E4%BD%8D%E8%AE%A4%E8%AF%86HBase%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%80%BC%E5%BE%97%E6%8B%A5%E6%9C%89%E7%9A%84NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2020/02/24/%E5%85%A8%E6%96%B9%E4%BD%8D%E8%AE%A4%E8%AF%86HBase%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%80%BC%E5%BE%97%E6%8B%A5%E6%9C%89%E7%9A%84NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2020-02-24T10:25:14.000Z</published>
    <updated>2020-02-24T11:20:51.383Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>前言：</strong>说起HBase这门技术，在认知上对于稍微接触或使用过它的人来讲，可能只是百千数据库中一个很普通的库，大概就像我对Redis的认知一样：缓存嘛！可对于HBase，我确实是带着某些感情在的。今日突然萌生了一个生趣的想法，想抛开技术的视角，从情感的角度，像写小说一样，写写这位老朋友，这可能会有点滑稽吧，不过我觉得很放松。《全方位认识HBase：一个值得拥有的NoSQL数据库》：从今天起，我们就暂且认为这是一本小说的名字吧！哈哈~</p></blockquote><a id="more"></a><p>其实我特别想做的一件事情，就是想让更多的人来认识并使用HBase这门地地道道的大数据栈技术，当然不为别的，主要原因还是HBase真的很棒很热，自己用着感觉真的好，不好的产品我怎么会推荐给你呢？毕竟HBase这家伙不会给我一分钱的广告费~</p><p>那首先，我想给大家分享的内容就是：在我刚接触HBase这位老朋友的时候根本不想去看的一些觉得没用的东西。什么呢？其实就是特别无聊又深奥的好像还不得不问的灵魂三问：我是谁？我从哪里来？我要到哪里去？</p><p>为什么想写写这个呢？真的好无聊啊~ 当然肯定不是我太无聊了，说实话，是因为对它真的有感情了，所以就想把它的前世今生全都介绍给你，可能算是一种情怀，也可能算是一种敬畏，也可能只是怕赶路的人忘了它是谁。</p><h3 id="我从哪里来？"><a href="#我从哪里来？" class="headerlink" title="我从哪里来？"></a>我从哪里来？</h3><p>我们知道，HBase出现于大数据背景之下，那么谈到这个问题，我们不得不提一下当年奠定了大数据算法基础的风靡全球的Google三篇论文，也称为Google的三驾马车：Google FS[2003]、MapReduce[2004]、BigTable[2006]。三篇论文中文版链接这里提供给大家，闲来没事可以看一看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;1EIhGR6gADm2BnEh5hW4KUA</span><br><span class="line">提取码：c1wb</span><br></pre></td></tr></table></figure><p>这三篇论文为何风靡全球呢？我们说随着大数据时代的到来，我们同样面临着大数据所带给我们的核心二问：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、海量数据如何存储？</span><br><span class="line">2、海量数据如何计算？</span><br><span class="line">3、海量结构化数据如何高效读写？</span><br></pre></td></tr></table></figure><p>然而，而谷歌公司在2003年至2006年发布的三篇论文则为解决两个问题提供了思路。</p><blockquote><p> “ 我们设计并实现了 Google GFS 文件系统，一个面向大规模数据密集型应用的、可伸缩的分布式文件系统。<br>GFS 虽然运行在廉价的普遍硬件设备上，但是它依然了提供灾难冗余的能力，为大量客户机提供了高性能的<br>服务。<br>…<br>GFS 完全满足了我们对存储的需求。”</p></blockquote><p>Google GFS 文件系统超前的设计思想，为解决大数据时代海量数据的存储提出了解决思路，同时对今后的分布式系统设计都提供了宝贵的指导意义。而MapReduce框架则解决了大数据时代海量数据如何计算的问题，虽然现在的Spark很火，但吃水不能忘了挖井人。</p><p>2006年，Google发布了第三篇重要论文。Bigtable 是一个分布式的结构化数据存储系统，它被设计用来处理海量数据：通常是分布在数千台普通服务器上的 PB 级的数据。Bigtable 的设计目的是可靠的处理 PB 级别的数据，并且能够部署到上千台机器上。用于解决Google内部海量结构化数据的存储以及高效读写问题。</p><p>也正是因为这三篇论文的发表，才有了而后的HDFS、MapReduce 和 HBase，才有了2015大数据元年。下面我们详细看一下Hadoop 家族的编年史，这里你大概也可以看出HBase在Hadoop家族中的地位。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">*   2002年10月，Doug Cutting和Mike Cafarella创建了开源网页爬虫项目Nutch。</span><br><span class="line"></span><br><span class="line">*   2003年10月，Google发表Google File System论文。</span><br><span class="line"></span><br><span class="line">*   2004年7月，Doug Cutting和Mike Cafarella在Nutch中实现了类似GFS的功能，即后来HDFS的前身。</span><br><span class="line"></span><br><span class="line">*   2004年10月，Google发表了MapReduce论文。</span><br><span class="line"></span><br><span class="line">*   2005年2月，Mike Cafarella在Nutch中实现了MapReduce的最初版本。</span><br><span class="line"></span><br><span class="line">*   2006年1月，Doug Cutting加入雅虎，Yahoo!提供一个专门的团队和资源将Hadoop发展成一个可在网络上运行的系统。</span><br><span class="line"></span><br><span class="line">*   2006年2月，Apache Hadoop项目正式启动以支持MapReduce和HDFS的独立发展。</span><br><span class="line"></span><br><span class="line">*   2006年3月，Yahoo!建设了第一个Hadoop集群用于开发。</span><br><span class="line"></span><br><span class="line">*   2006年4月，第一个Apache Hadoop发布。</span><br><span class="line"></span><br><span class="line">*   2006年11月，Google发表了Bigtable论文，这最终激发了HBase库的创建。</span><br><span class="line"></span><br><span class="line">*   2007年10月，第一个可用的HBase发布了。</span><br><span class="line"></span><br><span class="line">*   2008年1月，Hadoop成为Apache顶级项目。</span><br><span class="line"></span><br><span class="line">*   2008年1月，HBase成为 Hadoop 的子项目。</span><br><span class="line"></span><br><span class="line">*   2008年6月，Hadoop的第一个SQL框架——Hive成为了Hadoop的子项目。</span><br><span class="line"></span><br><span class="line">*   2009年7月 ，MapReduce 和 HDFS成为Hadoop项目的独立子项目。</span><br><span class="line"></span><br><span class="line">*   2009年7月 ，Avro 和 Chukwa 成为Hadoop新的子项目。</span><br><span class="line"></span><br><span class="line">*   2009年10月，首届Hadoop World大会在纽约召开。</span><br><span class="line"></span><br><span class="line">*   2010年5月 ，HBase脱离Hadoop项目，成为Apache顶级项目。</span><br><span class="line"></span><br><span class="line">*   2010年9月，Hive 脱离Hadoop，成为Apache顶级项目。</span><br><span class="line"></span><br><span class="line">*   2010年9月，Pig脱离Hadoop，成为Apache顶级项目。</span><br><span class="line"></span><br><span class="line">*   2011年1月，ZooKeeper 脱离Hadoop，成为Apache顶级项目。</span><br><span class="line"></span><br><span class="line">*   2012年8月，YARN成为Hadoop子项目。</span><br><span class="line"></span><br><span class="line">*   2012年10月，第一个Hadoop原生MPP查询引擎Impala加入到了Hadoop生态圈。</span><br><span class="line"></span><br><span class="line">*  2014年2月，Spark逐渐代替MapReduce成为Hadoop的缺省执行引擎，并成为Apache基金会顶级项目。</span><br><span class="line"></span><br><span class="line">*   2015年10月，Cloudera公布继HBase以后的第一个Hadoop原生存储替代方案——Kudu。</span><br><span class="line"></span><br><span class="line">*   2015年12月，Cloudera发起的Impala和Kudu项目加入Apache孵化器。</span><br></pre></td></tr></table></figure><p>好了，一张图向大家道一声晚安吧，挺晚了，该睡了~ 下一章我们再追问“我是谁？”的灵魂思考吧~</p><p><img src="https://i.loli.net/2020/02/24/UrTuh5aOBl4fqkD.png" alt="我从哪里来？"></p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://blog.csdn.net/lfq1532632051/article/details/53219558" target="_blank" rel="noopener">https://blog.csdn.net/lfq1532632051/article/details/53219558</a></p><p><strong>关注公众号【chan-ke】可查阅更多精彩文章~</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;前言：&lt;/strong&gt;说起HBase这门技术，在认知上对于稍微接触或使用过它的人来讲，可能只是百千数据库中一个很普通的库，大概就像我对Redis的认知一样：缓存嘛！可对于HBase，我确实是带着某些感情在的。今日突然萌生了一个生趣的想法，想抛开技术的视角，从情感的角度，像写小说一样，写写这位老朋友，这可能会有点滑稽吧，不过我觉得很放松。《全方位认识HBase：一个值得拥有的NoSQL数据库》：从今天起，我们就暂且认为这是一本小说的名字吧！哈哈~&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="2020" scheme="http://yoursite.com/tags/2020/"/>
    
      <category term="HBase" scheme="http://yoursite.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kylin 1.0中的混合模型</title>
    <link href="http://yoursite.com/2020/02/05/Apache-Kylin-1.0%E4%B8%AD%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/02/05/Apache-Kylin-1.0%E4%B8%AD%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-02-05T12:36:20.000Z</published>
    <updated>2020-02-16T05:02:07.168Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kylin v1.0引入了一个新的实现“混合模型”（也称为“动态模型”）; 这篇文章介绍了这个概念以及如何创建一个混合实例。</p><a id="more"></a><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>对于传入的SQL查询，Kylin选择一个（并且只有一个）实现来提供查询; 在“混合”之前，只有一种类型的实现向用户开放：Cube。也就是说，只有1个Cube被选中来回答查询;</p><p>现在我们来看一个示例案例。假设用户有一个名为“Cube_V1”的多维数据集，它已经建立了几个月; 现在，用户希望添加新的维度或指标以满足其业务需求; 于是他创建了一个名为“Cube_V2”的新立方体;</p><p>由于某些原因用户想要保留“Cube_V1”，并且期望从“Cube_V1”的结束日期开始构建“Cube_V2”; 可能的原因包括：</p><ul><li>历史源数据已从Hadoop中删除，从一开始就无法构建“Cube_V2”;</li><li>立方体很大，重建需要很长时间;</li><li>新维度/指标仅在某一天有效或应用;</li><li>当查询使用新的维度/指标时，用户感觉过去的结果为空。</li></ul><p>对于针对通用维度/指标的查询，用户期望扫描“Cube_V1”和“Cube_V2”以获得完整的结果集; 在这样的背景下，引入“混合模型”来解决这个问题。</p><h3 id="混合模型"><a href="#混合模型" class="headerlink" title="混合模型"></a>混合模型</h3><p>混合模型是一个新的实现，它是一个或多个其他实现（立方体）的组合; 见下图。</p><p><img src="http://upload-images.jianshu.io/upload_images/7875120-0d6377c5e4ed0ce9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>混合模型没有其真正的存储空间; 它就像在表格上的虚拟数据库视图一样; 混合实例充当委托者，将请求转发给其子实现，然后在从实例返回时合并结果。</p><h3 id="如何添加混合实例"><a href="#如何添加混合实例" class="headerlink" title="如何添加混合实例"></a>如何添加混合实例</h3><p>到目前为止，没有用于创建/编辑混合模型的UI界面; 如果有需要，您需要手动编辑Kylin元数据;</p><h4 id="第1步：做一个kylin元数据存储的备份"><a href="#第1步：做一个kylin元数据存储的备份" class="headerlink" title="第1步：做一个kylin元数据存储的备份"></a>第1步：做一个kylin元数据存储的备份</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export KYLIN_HOME&#x3D;&quot;&#x2F;path&#x2F;to&#x2F;kylin&quot;</span><br><span class="line"></span><br><span class="line">$KYLIN_HOME&#x2F;bin&#x2F;metastore.sh backup</span><br></pre></td></tr></table></figure><p>这将创建一个备份文件夹，假定它是$ KYLIN_HOME / metadata_backup / 2015-09-25 /</p><h4 id="第2步：创建子文件夹“hybrid”"><a href="#第2步：创建子文件夹“hybrid”" class="headerlink" title="第2步：创建子文件夹“hybrid”"></a>第2步：创建子文件夹“hybrid”</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $KYLIN_HOME&#x2F;metadata_backup&#x2F;2015-09-25&#x2F;hybrid</span><br></pre></td></tr></table></figure><h4 id="第3步：创建混合实例json文件"><a href="#第3步：创建混合实例json文件" class="headerlink" title="第3步：创建混合实例json文件"></a>第3步：创建混合实例json文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi $KYLIN_HOME&#x2F;metadata_backup&#x2F;2015-09-25&#x2F;hybrid&#x2F;my_hybrid.json</span><br></pre></td></tr></table></figure><p>像下面这样的输入内容，“名称”和“uuid”需要是唯一的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;uuid&quot;: &quot;9iiu8590-64b6-4367-8fb5-7500eb95fd9c&quot;,</span><br><span class="line">  &quot;name&quot;: &quot;my_hybrid&quot;,</span><br><span class="line">  &quot;realizations&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">           &quot;type&quot;: &quot;CUBE&quot;,</span><br><span class="line">           &quot;realization&quot;: &quot;Cube_V1&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">            &quot;type&quot;: &quot;CUBE&quot;,</span><br><span class="line">            &quot;realization&quot;: &quot;Cube_V2&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里“Cube_V1”和“Cube_V2”是你想合并的Cube名称。</p><h4 id="第4步：将混合实例添加到项目"><a href="#第4步：将混合实例添加到项目" class="headerlink" title="第4步：将混合实例添加到项目"></a>第4步：将混合实例添加到项目</h4><p>使用文本编辑器打开项目json文件（例如项目“default”）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi $KYLIN_HOME&#x2F;metadata_backup&#x2F;2015-09-25&#x2F;project&#x2F;default.json</span><br></pre></td></tr></table></figure><p>在“realizations”数组中，添加一个如下所示的条目，类型需要是“HYBRID”，“实现”是混合实例的名称：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;my_hybrid&quot;,</span><br><span class="line">  &quot;type&quot;: &quot;HYBRID&quot;,</span><br><span class="line">  &quot;realization&quot;: &quot;my_hybrid&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="第5步：上传元数据"><a href="#第5步：上传元数据" class="headerlink" title="第5步：上传元数据"></a>第5步：上传元数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$KYLIN_HOME&#x2F;bin&#x2F;metastore.sh restore $KYLIN_HOME&#x2F;metadata_backup&#x2F;2015-09-25&#x2F;</span><br></pre></td></tr></table></figure><p>请注意，“restore”操作会将元数据从本地上传到远程hbase store，这可能会覆盖远程中的更改; 因此，请在此期间没有从Kylin服务器更改元数据（无 build job，无Cube 创建/更新等）时执行此操作，或者在运行“restore”之前仅将已更改的文件提取到空的本地文件夹。</p><h4 id="第6步：重新加载元数据"><a href="#第6步：重新加载元数据" class="headerlink" title="第6步：重新加载元数据"></a>第6步：重新加载元数据</h4><p>重新启动Kylin服务器，或在Kylin Web UI的“管理”标签中点击“重新加载元数据”以加载更改; 理想情况下，混合动力车将开始工作; 你可以通过编写一些SQL来做一些验证。</p><h3 id="常问问题"><a href="#常问问题" class="headerlink" title="常问问题"></a>常问问题</h3><p><strong>问题1</strong>：混合模型何时被调用来回答客户端SQL查询？<br>如果混合模型中存在一个立方体可以回答查询，则将选择混合体;</p><p><strong>问题2</strong>：混合模型如何回答一个查询？<br>Hybrid将把查询委托给它的每个子实现; 如果一个子多维数据集能够执行此查询（匹配所有维度/指标），它会将结果返回到混合模式，否则将被跳过; 最后，查询引擎会在返回给用户之前聚合来自混合的数据;</p><p><strong>问题3</strong>：混合模型检查日期/时间重复吗？<br>不检查; 这个需要用户确保混合中的立方体不具有重复的日期/时间范围; 例如，“Cube_V1”在2015-9-20（不含）结束，“Cube_V2”应从2015-9-20（含）开始;</p><p><strong>问题4</strong>：混合模型会限制具有相同数据模型的子立方体吗？<br>不会; 为了提供灵活性，Hybrid不检查子立方体的事实表/查找表和连接条件是否相同; 但用户应该明白他们在做什么以避免意外的行为。</p><p><strong>问题5</strong>：混合模型中是否可以包含子hybrid ？<br>不能; 没有这个必要; 到目前为止，假设所有的Child都是Cube;</p><p><strong>问题6</strong>：我可以使用混合加入多个立方体吗？<br>不能; 混合模型的目的是连接历史Cube和新Cube，类似“union”而不是“join”;</p><p><strong>问题7</strong>：如果子立方体被禁用，它是否会通过混合动力进行扫描？<br>不会; 混合实例会在发送查询之前检查子实现的状态; 所以如果立方体被禁用，它将不会被扫描。</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p><strong>1.</strong> 必须在构建好新的cube后备份元数据，从而进行修改恢复<br><strong>2.</strong> 修改完元数据后必须重启kylin集群或者重新载入元数据，否则报错（Overwriting conflict /project/<strong><em>.json, expect old TS 1525758479256, but it is 152576031000）<br>*</em>3.</strong> 若修改视图表（如添加字段），修改完毕后要reload一下<br><strong>4.</strong> Purge Cube 后Hbase中存储的计算结果不会被删除，Hbase可查<br><strong>5.</strong> 新Cube不能build旧Cube已经build过的相同日期的数据，若修复历史数据，需先删除旧的Segment<br><strong>6.</strong> 若想修改字段名，必须Perge Cube，Hybrid 无法满足需求<br><strong>7.</strong> 新增字段在build完毕后Kylin表才会更新字段信息</p><h3 id="清除Hbase无用数据表"><a href="#清除Hbase无用数据表" class="headerlink" title="清除Hbase无用数据表"></a>清除Hbase无用数据表</h3><p>当我们对cube执行purge/drop/merge时，一些HBase的表可能会保留在HBase中，而这些表不再被查询，尽管Kylin会做一些自动的垃圾回收，但是它可能不会覆盖所有方面，所以需要我们能够每隔一段时间做一些离线存储的清理工作。具体步骤如下：</p><p><strong>1.</strong> 检查哪些资源需要被清理，这个操作不会删除任何内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$&#123;KYLIN_HOME&#125;&#x2F;bin&#x2F;kylin.sh org.apache.kylin.storage.hbase.util.StorageCleanupJob --delete false</span><br></pre></td></tr></table></figure><p><strong>2.</strong> 根据上面的输出结果，挑选一两个资源看看是否是不再需要的。接着，在上面的命令基础上添加“–delete true”选项，开始执行清理操作，命令执行完成后，中间的HDFS文和盒HTables表就被删除了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kylin v1.0引入了一个新的实现“混合模型”（也称为“动态模型”）; 这篇文章介绍了这个概念以及如何创建一个混合实例。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kylin" scheme="http://yoursite.com/categories/Kylin/"/>
    
    
      <category term="Kylin" scheme="http://yoursite.com/tags/Kylin/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="2020" scheme="http://yoursite.com/tags/2020/"/>
    
  </entry>
  
  <entry>
    <title>基于HBase构建千亿级文本数据相似度计算与快速去重系统</title>
    <link href="http://yoursite.com/2020/02/05/%E5%9F%BA%E4%BA%8EHBase%E6%9E%84%E5%BB%BA%E5%8D%83%E4%BA%BF%E7%BA%A7%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%BF%AB%E9%80%9F%E5%8E%BB%E9%87%8D%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2020/02/05/%E5%9F%BA%E4%BA%8EHBase%E6%9E%84%E5%BB%BA%E5%8D%83%E4%BA%BF%E7%BA%A7%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%BF%AB%E9%80%9F%E5%8E%BB%E9%87%8D%E7%B3%BB%E7%BB%9F/</id>
    <published>2020-02-05T12:34:40.000Z</published>
    <updated>2020-02-24T10:55:35.532Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>随着大数据时代的到来，数据信息在给我们生活带来便利的同时，同样也给我们带来了一系列的考验与挑战。本文主要介绍了基于 Apache HBase 与 Google SimHash 等多种算法共同实现的一套支持百亿级文本数据相似度计算与快速去重系统的设计与实现。该方案在公司业务层面彻底解决了多主题海量文本数据所面临的存储与计算慢的问题。</p><a id="more"></a><h3 id="一-面临的问题"><a href="#一-面临的问题" class="headerlink" title="一. 面临的问题"></a>一. 面临的问题</h3><h4 id="1-如何选择文本的相似度计算或去重算法？"><a href="#1-如何选择文本的相似度计算或去重算法？" class="headerlink" title="1. 如何选择文本的相似度计算或去重算法？"></a>1. 如何选择文本的相似度计算或去重算法？</h4><p>常见的有余弦夹角算法、欧式距离、Jaccard 相似度、最长公共子串、编辑距离等。这些算法对于待比较的文本数据不多时还比较好用，但在海量数据背景下，如果每天产生的数据以千万计算，我们如何对于这些海量千万级的数据进行高效的合并去重和相似度计算呢？</p><h4 id="2-如何实现快速计算文本相似度或去重呢？"><a href="#2-如何实现快速计算文本相似度或去重呢？" class="headerlink" title="2. 如何实现快速计算文本相似度或去重呢？"></a>2. 如何实现快速计算文本相似度或去重呢？</h4><p>如果我们选好了相似度计算和去重的相关算法，那我们怎么去做呢？如果待比较的文本数据少，我们简单遍历所有文本进行比较即可，那对于巨大的数据集我们该怎么办呢？遍历很明显是不可取的。</p><h4 id="3-海量数据的存储与快速读写"><a href="#3-海量数据的存储与快速读写" class="headerlink" title="3. 海量数据的存储与快速读写"></a>3. 海量数据的存储与快速读写</h4><h3 id="二-SimHash-算法引入"><a href="#二-SimHash-算法引入" class="headerlink" title="二. SimHash 算法引入"></a>二. SimHash 算法引入</h3><p>基于问题一，我们引入了 SimHash 算法来实现海量文本的相似度计算与快速去重。下面我们简单了解下该算法。</p><h4 id="1-局部敏感哈希"><a href="#1-局部敏感哈希" class="headerlink" title="1. 局部敏感哈希"></a>1. 局部敏感哈希</h4><p>在介绍 SimHash 算法之前，我们先简单介绍下局部敏感哈希是什么。局部敏感哈希的基本思想类似于一种空间域转换思想，LSH 算法基于一个假设，如果两个文本在原有的数据空间是相似的，那么分别经过哈希函数转换以后的它们也具有很高的相似度；相反，如果它们本身是不相似的，那么经过转换后它们应仍不具有相似性。</p><p> 局部敏感哈希的最大特点就在于保持数据的相似性，举一个小小的例子说明一下：对A文章微调后我们称其为B文章（可能只是多了一个‘的’字），如果此时我们计算两篇文章的 MD5 值，那么必将大相径庭。而局部敏感哈希的好处是经过哈希函数转换后的值也只是发生了微小的变化，即如果两篇文章相似度很高，那么在算法转换后其相似度也会很高。</p><p>MinHash 与 SimHash 算法都属于局部敏感哈希，一般情况若每个 Feature 无权重，则 MinHash 效果优于 SimHash 有权重时 SimHash 合适。长文本使用 Simhash 效果很好，短文本使用 Simhash 准备度不高。</p><h4 id="2-SimHash-算法"><a href="#2-SimHash-算法" class="headerlink" title="2. SimHash 算法"></a>2. SimHash 算法</h4><p>SimHash 是 Google 在2007年发表的论文《Detecting Near-Duplicates for Web Crawling 》中提到的一种指纹生成算法或者叫指纹提取算法，被 Google 广泛应用在亿级的网页去重的 Job 中，其主要思想是降维，经过simhash降维后，可能仅仅得到一个长度为32或64位的二进制由01组成的字符串。而一维查询则是非常快速的。</p><p>SimHash的工作原理我们这里略过，大家可以简单理解为：我们可以利用SimHash算法为每一个网页/文章生成一个长度为32或64位的二进制由01组成的字符串（向量指纹），形如：1000010010101101111111100000101011010001001111100001001011001011。</p><h4 id="3-海明距离"><a href="#3-海明距离" class="headerlink" title="3. 海明距离"></a>3. 海明距离</h4><p>两个码字的对应比特取值不同的比特数称为这两个码字的海明距离。在一个有效编码集中,任意两个码字的海明距离的最小值称为该编码集的海明距离。举例如下：10101和00110从第一位开始依次有第一位、第四、第五位不同，则海明距离为3。</p><p>在 google 的论文给出的数据中，64位的签名，在海明距离为3的情况下，可认为两篇文档是相似的或者是重复的，当然这个值只是参考值。</p><p>这样，基于 SimHash 算法，我们就可以将百亿千亿级的高维特征文章转变为一维字符串后再通过计算其海明距离判断网页/文章的相似度，可想效率必将大大提高。</p><h3 id="三-效率问题"><a href="#三-效率问题" class="headerlink" title="三. 效率问题"></a>三. 效率问题</h3><p>到这里相似度问题基本解决，但是按这个思路，在海量数据几百亿的数量下，效率问题还是没有解决的，因为数据是不断添加进来的，不可能每来一条数据，都要和全库的数据做一次比较，按照这种思路，处理速度会越来越慢，线性增长。</p><p>这里，我们要引入一个新的概念：<strong>抽屉原理</strong>，也称鸽巢原理。下面我们简单举例说一下：</p><p>桌子上有四个苹果，但只有三个抽屉，如果要将四个苹果放入三个抽屉里，那么必然有一个抽屉中放入了两个苹果。如果每个抽屉代表一个集合，每一个苹果就可以代表一个元素，假如有n+1个元素放到n个集合中去，其中必定有一个集合里至少有两个元素。</p><p>抽屉原理就是这么简单，那如果用它来解决我们海量数据的遍历问题呢？</p><p>针对海量数据的去重效率，我们可以将64位指纹，切分为4份16位的数据块，根据抽屉原理在海明距离为3的情况，如果两个文档相似，那么它必有一个块的数据是相等的。</p><p>那也就是说，我们可以以某文本的 SimHash 的每个16位截断指纹为 Key，Value 为 Key 相等时文本的 SimHash 集合存入 K-V 数据库即可，查询时候，精确匹配这个指纹的4个16位截断指纹所对应的4个 SimHash 集合即可。</p><p>如此，假设样本库，有2^37 条数据（1375亿数据），假设数据均匀分布，则每个16位（16个01数字随机组成的组合为2^16 个）倒排返回的最大数量为<br>(2^37) * 4 / (2^16) =8388608个候选结果，4个16位截断索引，总的结果为：4*8388608=33554432，约为3356万，通过<br>这样一来的降维处理，原来需要比较1375亿次，现在只需要比较3356万次即可得到结果，这样以来大大提升了计算效率。</p><p>根据网上测试数据显示，普通 PC 比较1000万次海明距离大约需要 300ms，也就是说3356万次（1375亿数据）只需花费3356/1000*0.3=1.0068s。那也就是说对于千亿级文本数据（如果每个文本1kb，约100TB数据）的相似度计算与去重工作我们最多只需要一秒的时间即可得出结果。</p><h3 id="四-HBase-存储设计"><a href="#四-HBase-存储设计" class="headerlink" title="四. HBase 存储设计"></a>四. HBase 存储设计</h3><p>饶了这么大一周，我们终于将需要讲明的理论知识给大家过了一遍。为了阐述的尽量清晰易懂，文中很多理论知识的理解借鉴了大量博主大牛的博客，原文链接已在文末附上，有不太明白的地方快快跪拜大牛们的博客吧，哈哈！</p><p>下面我们着重介绍一下 HBase 存储表的设计与实现。</p><p>基于上文我们可以大概知道，如果将64位指纹平分四份，海明距离取3，那么必有一段16位截取指纹的数据是相等的。而每一段16位截取指纹对应一个64位指纹集合，且该集合中的每个64位指纹必有一段16位截取指纹与该段16位截取指纹重合。我们可以简单表示(以8位非01指纹举例)为：</p><table><thead><tr><th>key</th><th>value(set)</th></tr></thead><tbody><tr><td>12</td><td>[12345678,12345679]</td></tr><tr><td>23</td><td>[12345678,12345679,23456789]</td></tr></tbody></table><p>那如果基于 HBase 去实现的话，我们大概对比三种可能的设计方案。</p><h4 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h4><p>以 16 位指纹作为 HBase 数据表的行键，将每一个与之可能相似的64位指纹作为 HBase 的列，列值存文章id值，即构建一张大宽表。如下表所示(以8位非01指纹举例)：</p><table><thead><tr><th>rowkey</th><th>column1</th><th>column2</th><th>column3</th><th>…</th></tr></thead></table><p>实际数据表可能是这个样子：</p><table><thead><tr><th>rowkey</th><th>12345678</th><th>32234567</th><th>23456789</th><th>12456789</th><th>…</th></tr></thead><tbody><tr><td>12</td><td>1102101</td><td></td><td></td><td>1102102</td><td>…</td></tr><tr><td>23</td><td></td><td>1102104</td><td>1102105</td><td></td><td>…</td></tr><tr><td>34</td><td>1102106</td><td></td><td></td><td></td><td>…</td></tr></tbody></table><p>那其实这样设计表的话该 HBase 表 Rowkey 的个数就是一个确定的数值：16个01数字随机组成的组合为2^16 个。也就是共2^16=65536行。 列的个数其实也是固定的，即2^64=184467440737亿万列。</p><p>此时，比如说我们比较56431234与库中所有文本的相似度，只需拉去rowkey in (56,43,12,34) 四行数据遍历每行列，由于 HBase 空值不进行存储，所有只会遍历存在值的列名。</p><p>由上文我们计算出1350亿数据如果平均分布的话每行大约有839万列，且不说我们的数据量可能远远大于千亿级别，也不说以64位字符串作为列名所占的存储空间有多大，单单千亿级数据量 HBase 每行就大约839万列，虽说HBase号称支持千万行百万列数据存储，但总归还是设计太不合理。数据不会理想化均匀分布，总列数高达184467440737亿万列也令人堪忧。</p><h4 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h4><p>以 16 位指纹与64位指纹拼接后作为 HBase 数据表的行键，该表只有一列，列值存文章id值，即构建一张大长表。如下表所示(以8位非01指纹举例)：</p><table><thead><tr><th>rowkey</th><th>id</th></tr></thead></table><p>实际数据表可能是这个样子：</p><table><thead><tr><th>rowkey</th><th>id</th></tr></thead><tbody><tr><td>12_12345678</td><td>1</td></tr><tr><td>34_12345678</td><td>1</td></tr><tr><td>56_12345678</td><td>1</td></tr><tr><td>78_12345678</td><td>1</td></tr><tr><td>34_22345678</td><td>2</td></tr><tr><td>23_12235678</td><td>3</td></tr></tbody></table><p>如此设计感觉要比第一种方法要好一些，每一篇文章会被存为四行。但同样有诸多缺点，一是 Rowkey 过长，二是即便我们通过某种转变设计解决了问题一，那获取数据时我们也只能将 Get 请求转为四个Scan并发扫描+StartEnKey 去扫描表获取数据。当然，如果想实现顺序扫描还可能存在热点问题。在存储上，也造成了数据大量冗余。</p><h4 id="方案三"><a href="#方案三" class="headerlink" title="方案三"></a>方案三</h4><p>在真实生产环境中，我们采取该方案来避免上述两个方案中出现的问题与不足。下面简单介绍一下（如果您有更好更优的方案，欢迎留言，先表示感谢！）</p><p>简言之呢，就是自己在 HBase 端维护了一个 Set 集合（协处理器），并以 Json 串进行存储，格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;64SimHash1&quot;:&quot;id1&quot;,</span><br><span class="line">    &quot;64SimHash2&quot;:&quot;id2&quot;,</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基于公司存在多种主题类型的文本数据，且互相隔离，去重与相似度计算也是分主题进行，我们的 Rowkey 设计大致如下：</p><p>Rowkey = HashNumber_ContentType_16SimHash  (共24位)</p><ul><li>HashNumber： 为防热点，对表进行Hash预分区（64个预分区），占2个字符<br>计算公式如下：String.format(“%02x”, Math.abs(key.hashCode()) % 64)</li><li>ContentType ：内容主题类型，占4个字符</li><li>16SimHash： 16位 SimHash 截取指纹，由01组成</li></ul><p>表结构大致如下：</p><table><thead><tr><th>rowkey</th><th>si</th><th>s0</th><th>s1</th><th>s2</th><th>s3</th><th>…</th></tr></thead><tbody><tr><td>01_news_010101010101010101</td><td>value</td><td>1</td><td>Json 串</td><td></td><td></td><td>…</td></tr><tr><td>02_news_010101010101010110</td><td>value</td><td>2</td><td>Json 串</td><td>Json 串</td><td></td><td>…</td></tr><tr><td>03_news_100101010101010110</td><td>value</td><td>3</td><td>Json 串</td><td>Json 串</td><td>Json 串</td><td>…</td></tr><tr><td>01_xbbs_010101010101010101</td><td>value</td><td>1</td><td>Json 串</td><td></td><td></td><td>…</td></tr></tbody></table><p>si：客户端传递过来的欲存储的值，由64位 Simhash 与 Id 通过双下划线拼接而成，诸如 Simhash__Id 的形式。<br>s0：记录该行数据共有多少个 Set 集合，每一个 Set 集合存储10000个K-V对儿（约1MB）。<br>s1：第一个 Set 集合，Json 串存储，如果 Size &gt; 10000 ，之后来的数据将存入s2。<br>s2：以此类推。</p><p>当然最核心的部分是s1/s2/s3 中 Json 串中要排重。最简单的办法无非是每次存入数据前先将所有 Set 集合中的数据读到客户端，将欲存的数据与集合中所有数据比对后再次插入。这将带来大量往返IO开销，影响写性能。因此，我们在此引入了 HBase 协处理器技术来规避这个问题，即在服务端完成所有排重操作。大致代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">package com.learn.share.scenarios.observers;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import com.google.gson.Gson;</span><br><span class="line">import com.google.gson.JsonObject;</span><br><span class="line">import org.apache.commons.lang.StringUtils;</span><br><span class="line">import org.apache.hadoop.hbase.Cell;</span><br><span class="line">import org.apache.hadoop.hbase.CellUtil;</span><br><span class="line">import org.apache.hadoop.hbase.CoprocessorEnvironment;</span><br><span class="line">import org.apache.hadoop.hbase.client.Durability;</span><br><span class="line">import org.apache.hadoop.hbase.client.Get;</span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.client.Result;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.ObserverContext;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;</span><br><span class="line">import org.apache.hadoop.hbase.regionserver.wal.WALEdit;</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> *  基于协处理器构建百亿级文本去重系统</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class HBaseSimHashSetBuildSystem extends BaseRegionObserver &#123;</span><br><span class="line"></span><br><span class="line">    private Logger logger &#x3D; LoggerFactory.getLogger(HBaseSimHashSetBuildSystem.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void start(CoprocessorEnvironment e) throws IOException &#123;</span><br><span class="line">        logger.info(&quot;Coprocessor opration start...&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     *</span><br><span class="line">     * @param e</span><br><span class="line">     * @param put</span><br><span class="line">     * @param edit</span><br><span class="line">     * @param durability</span><br><span class="line">     * @throws IOException</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123;</span><br><span class="line">        &#x2F;&#x2F; test flag</span><br><span class="line">        logger.info(&quot;do something before Put Opration...&quot;);</span><br><span class="line"></span><br><span class="line">        List&lt;Cell&gt; cells &#x3D; put.get(Bytes.toBytes(&quot;f&quot;), Bytes.toBytes(&quot;si&quot;));</span><br><span class="line">        if (cells &#x3D;&#x3D; null || cells.size() &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        String simhash__itemid &#x3D; Bytes.toString(CellUtil.cloneValue(cells.get(0)));</span><br><span class="line">        if (StringUtils.isEmpty(simhash__itemid)||simhash__itemid.split(&quot;__&quot;).length!&#x3D;2)&#123;</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        String simhash &#x3D; simhash__itemid.trim().split(&quot;__&quot;)[0];</span><br><span class="line">        String itemid &#x3D; simhash__itemid.trim().split(&quot;__&quot;)[1];</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 获取Put Rowkey</span><br><span class="line">        byte[] row &#x3D; put.getRow();</span><br><span class="line">        &#x2F;&#x2F; 通过Rowkey构造Get对象</span><br><span class="line">        Get get &#x3D; new Get(row);</span><br><span class="line">        get.setMaxVersions(1);</span><br><span class="line">        get.addFamily(Bytes.toBytes(&quot;f&quot;));</span><br><span class="line">        Result result &#x3D; e.getEnvironment().getRegion().get(get);</span><br><span class="line">        Cell columnCell &#x3D; result.getColumnLatestCell(Bytes.toBytes(&quot;f&quot;), Bytes.toBytes(&quot;s0&quot;)); &#x2F;&#x2F; set size</span><br><span class="line">        if (columnCell &#x3D;&#x3D; null) &#123;</span><br><span class="line">            &#x2F;&#x2F; 第一次存储数据，将size初始化为1</span><br><span class="line">            logger.info(&quot;第一次存储数据，将size初始化为1&quot;);</span><br><span class="line"></span><br><span class="line">            JsonObject jsonObject &#x3D; new JsonObject();</span><br><span class="line">            jsonObject.addProperty(simhash,itemid);</span><br><span class="line">            Gson gson &#x3D; new Gson();</span><br><span class="line">            String json &#x3D; gson.toJson(jsonObject);</span><br><span class="line"></span><br><span class="line">            put.addColumn(Bytes.toBytes(&quot;f&quot;),Bytes.toBytes(&quot;s1&quot;), Bytes.toBytes(json)); &#x2F;&#x2F; json 数组</span><br><span class="line">            put.addColumn(Bytes.toBytes(&quot;f&quot;),Bytes.toBytes(&quot;s0&quot;), Bytes.toBytes(&quot;1&quot;));  &#x2F;&#x2F; 初始化</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            byte[] sizebyte &#x3D; CellUtil.cloneValue(columnCell);</span><br><span class="line">            int size &#x3D; Integer.parseInt(Bytes.toString(sizebyte));</span><br><span class="line">            logger.info(&quot;非第一次存储数据 ----&gt; Rowkey &#96;&quot;+Bytes.toString(row)+&quot;&#96; simhash set size is : &quot;+size +&quot;, the current value is : &quot;+simhash__itemid);</span><br><span class="line">            for (int i &#x3D; 1; i &lt;&#x3D; size; i++) &#123;</span><br><span class="line">                Cell cell1 &#x3D; result.getColumnLatestCell(Bytes.toBytes(&quot;f&quot;), Bytes.toBytes(&quot;s&quot;+i));</span><br><span class="line">                String jsonBefore &#x3D; Bytes.toString(CellUtil.cloneValue(cell1));</span><br><span class="line">                Gson gson &#x3D; new Gson();</span><br><span class="line">                JsonObject jsonObject &#x3D; gson.fromJson(jsonBefore, JsonObject.class);</span><br><span class="line">                int sizeBefore &#x3D; jsonObject.entrySet().size();</span><br><span class="line">                if(i&#x3D;&#x3D;size)&#123;</span><br><span class="line">                    if(!jsonObject.has(simhash))&#123;</span><br><span class="line">                        if (sizeBefore&#x3D;&#x3D;10000)&#123;</span><br><span class="line">                            JsonObject jsonone &#x3D; new JsonObject();</span><br><span class="line">                            jsonone.addProperty(simhash,itemid);</span><br><span class="line">                            String jsonstrone &#x3D; gson.toJson(jsonone);</span><br><span class="line">                            put.addColumn(Bytes.toBytes(&quot;f&quot;),Bytes.toBytes(&quot;s&quot;+(size+1)), Bytes.toBytes(jsonstrone)); &#x2F;&#x2F; json 数组</span><br><span class="line">                            put.addColumn(Bytes.toBytes(&quot;f&quot;),Bytes.toBytes(&quot;s0&quot;), Bytes.toBytes((size+1)+&quot;&quot;));  &#x2F;&#x2F; 初始化</span><br><span class="line">                        &#125;else &#123;</span><br><span class="line">                            jsonObject.addProperty(simhash,itemid);</span><br><span class="line">                            String jsonAfter &#x3D; gson.toJson(jsonObject);</span><br><span class="line">                            put.addColumn(Bytes.toBytes(&quot;f&quot;),Bytes.toBytes(&quot;s&quot;+size), Bytes.toBytes(jsonAfter)); &#x2F;&#x2F; json 数组</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;else &#123;</span><br><span class="line">                        return;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;else&#123;</span><br><span class="line">                    if(!jsonObject.has(simhash))&#123;</span><br><span class="line">                        continue;</span><br><span class="line">                    &#125;else &#123;</span><br><span class="line">                        return;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如此，当我们需要对某一文本指纹与库中数据进行比对时，只需一个Table.Get(List<Get>) 操作即可返回所有的数据，然后基于s0依次获取各个 Set 集合中的数据即可。</p><p>下面我们算一笔账，假设我们某主题类型数据依然有 2^37 条数据（1375亿数据），假设数据均匀分布，则每个16位（16个01数字随机组成的组合为2^16 个）倒排返回的最大数量为 (2^37) * 4 / (2^16) =8388608个候选结果，即每行约839个 Set 集合，每个Set 集合大约1M 的话，数据存储量也必然不会太大。</p><p>你如果有十种不同主题的数据，HBase 行数无非也才 (2^16)*10 = 655360 行而已。</p><p>如果再加上 Snappy 压缩呢？<br>如果再加上 Fast-Diff 编码呢？<br>如果再开启 Mob 对象存储呢？ 每个 Set 是不是可以存10万个键值对？每行只需90个 Set 集合。</p><p>也或许，如果数据量小的话，使用 Redis 是不是更好呢？</p><p>总之，优化完善和不完美的地方还很多，本文也就简单叙述到此，如果您有好的建议或是不同看法，欢迎留言哦！感恩~ 晚安各位~~</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/u010454030/article/details/49102565" target="_blank" rel="noopener">1. https://blog.csdn.net/u010454030/article/details/49102565</a><br><a href="http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity2-html.html" target="_blank" rel="noopener">2. http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity2-html.html</a><br><a href="https://cloud.tencent.com/developer/news/218062" target="_blank" rel="noopener">3. https://cloud.tencent.com/developer/news/218062</a><br><a href="https://blog.csdn.net/qq_36142114/article/details/80540303" target="_blank" rel="noopener">4. https://blog.csdn.net/qq_36142114/article/details/80540303</a><br><a href="https://blog.csdn.net/u011467621/article/details/49685107" target="_blank" rel="noopener">5. https://blog.csdn.net/u011467621/article/details/49685107</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;随着大数据时代的到来，数据信息在给我们生活带来便利的同时，同样也给我们带来了一系列的考验与挑战。本文主要介绍了基于 Apache HBase 与 Google SimHash 等多种算法共同实现的一套支持百亿级文本数据相似度计算与快速去重系统的设计与实现。该方案在公司业务层面彻底解决了多主题海量文本数据所面临的存储与计算慢的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="HBase" scheme="http://yoursite.com/categories/HBase/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="2020" scheme="http://yoursite.com/tags/2020/"/>
    
      <category term="HBase" scheme="http://yoursite.com/tags/HBase/"/>
    
      <category term="SimHash" scheme="http://yoursite.com/tags/SimHash/"/>
    
      <category term="文本相似度" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
  </entry>
  
</feed>
